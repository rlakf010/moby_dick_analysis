{"cells":[{"source":"![mobydick](mobydick.jpg)","metadata":{},"id":"b1309988-b429-4fb0-8c4c-193582dbec93","cell_type":"markdown"},{"source":"In this workspace, you'll scrape the novel Moby Dick from the website [Project Gutenberg](https://www.gutenberg.org/) (which contains a large corpus of books) using the Python `requests` package. You'll extract words from this web data using `BeautifulSoup` before analyzing the distribution of words using the Natural Language ToolKit (`nltk`) and `Counter`.\n\nThe Data Science pipeline you'll build in this workspace can be used to visualize the word frequency distributions of any novel you can find on Project Gutenberg.","metadata":{},"id":"611e416c-70e7-478a-a3c8-e54f3fdb4a7f","cell_type":"markdown"},{"source":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')\n\n# Start coding here... ","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionTime":16,"lastSuccessfullyExecutedCode":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')\n\n# Start coding here... ","executionCancelledAt":null,"lastExecutedAt":1767620154321,"lastExecutedByKernel":"a40ed154-03ae-4f9b-8655-adec26ff10f2","lastScheduledRunId":null,"outputsMetadata":{"0":{"height":59,"type":"stream"}}},"id":"15b5f52f-fd9b-4f0e-9fcc-f7733022c7c0","cell_type":"code","execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"},{"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{},"execution_count":18}]},{"source":"# 라이브러리 임포트\nimport requests\nfrom bs4 import BeautifulSoup\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport nltk\n\n# NLTK 불용어 다운로드 (최초 1회)\nnltk.download('stopwords')\n\n# 1. HTML 파일 요청 (객체 이름: r)\nurl = \"https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\"\nr = requests.get(url)\nr.encoding = 'utf-8'\nhtml = r.text\n\n# 2. BeautifulSoup 객체 생성 (객체 이름: html_soup)\nhtml_soup = BeautifulSoup(html, \"html.parser\")\n\n# 3. 텍스트 추출 (객체 이름: moby_text)\nmoby_text = html_soup.get_text()\n\n# 4. 정규표현식 토크나이저 생성\ntokenizer = RegexpTokenizer(r'\\w+')\ntokens = tokenizer.tokenize(moby_text)\n\n# 5. 소문자로 변환 (객체 이름: words)\nwords = [word.lower() for word in tokens]\n\n# 6. 불용어 제거 (객체 이름: words_no_stop)\nstop_words = stopwords.words('english')\nwords_no_stop = [\n    word for word in words\n    if word not in stop_words\n]\n\n# 7. Counter 객체 생성 (객체 이름: count) ⭐ 중요\ncount = Counter(words_no_stop)\n\n# 8. 가장 많이 등장한 단어 10개 추출\ntop_ten = count.most_common(10)\n\n# 결과 출력\nprint(top_ten)\n\n## Results Interpretation\n\nThe most frequent words in *Moby Dick* reveal the main themes and focus of the novel.\n\nWords such as **\"whale\"**, **\"sea\"**, **\"ship\"**, and **\"captain\"** appear repeatedly, indicating that the story strongly revolves around maritime life and the obsessive pursuit of the whale.\n\nThe frequent appearance of **\"ahab\"** highlights the central role of Captain Ahab and emphasizes his dominance in the narrative.  \nThis supports the idea that the novel is not only an adventure story but also a deep psychological exploration of obsession and revenge.\n\n## Analytical Insight\n\nBy removing stopwords and analyzing word frequencies, we can focus on meaningful terms that carry thematic significance.  \nThis approach helps identify key motifs without manually reading the entire text.\n\nFor example, the dominance of words related to the sea and the whale reflects the novel’s intense focus on nature, fate, and human struggle.  \nSuch text analysis techniques are useful in literary studies, content analysis, and large-scale document processing.\n\n## Conclusion\n\nThis project demonstrates how Python can be used to:\n- Collect data from the web\n- Clean and preprocess raw text\n- Perform basic natural language processing\n- Extract meaningful insights from large documents\n\nThe same workflow can be applied to news articles, customer reviews, social media data, and other real-world text datasets.  \nThis makes it a strong foundation for further work in data analysis, NLP, and machine learning.\n\n## One-Line Summary\n\nThis analysis shows how simple NLP techniques can uncover thematic patterns in classic literature using Python.\n","metadata":{"executionCancelledAt":null,"executionTime":937,"lastExecutedAt":1767620158077,"lastExecutedByKernel":"a40ed154-03ae-4f9b-8655-adec26ff10f2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 라이브러리 임포트\nimport requests\nfrom bs4 import BeautifulSoup\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport nltk\n\n# NLTK 불용어 다운로드 (최초 1회)\nnltk.download('stopwords')\n\n# 1. HTML 파일 요청 (객체 이름: r)\nurl = \"https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\"\nr = requests.get(url)\nr.encoding = 'utf-8'\nhtml = r.text\n\n# 2. BeautifulSoup 객체 생성 (객체 이름: html_soup)\nhtml_soup = BeautifulSoup(html, \"html.parser\")\n\n# 3. 텍스트 추출 (객체 이름: moby_text)\nmoby_text = html_soup.get_text()\n\n# 4. 정규표현식 토크나이저 생성\ntokenizer = RegexpTokenizer(r'\\w+')\ntokens = tokenizer.tokenize(moby_text)\n\n# 5. 소문자로 변환 (객체 이름: words)\nwords = [word.lower() for word in tokens]\n\n# 6. 불용어 제거 (객체 이름: words_no_stop)\nstop_words = stopwords.words('english')\nwords_no_stop = [\n    word for word in words\n    if word not in stop_words\n]\n\n# 7. Counter 객체 생성 (객체 이름: count) ⭐ 중요\ncount = Counter(words_no_stop)\n\n# 8. 가장 많이 등장한 단어 10개 추출\ntop_ten = count.most_common(10)\n\n# 결과 출력\nprint(top_ten)\n","outputsMetadata":{"0":{"height":59,"type":"stream"},"1":{"height":59,"type":"stream"}}},"cell_type":"code","id":"77421a1e-4e4e-41ec-951e-f97bf1f8136f","outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"},{"output_type":"stream","name":"stdout","text":"[('whale', 1246), ('one', 925), ('like', 647), ('upon', 568), ('man', 527), ('ship', 519), ('ahab', 517), ('ye', 473), ('sea', 455), ('old', 452)]\n"}],"execution_count":22}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}